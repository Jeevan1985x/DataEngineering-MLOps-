{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Perform data processing with SageMaker Processing\n",
    "\n",
    "In this notebook, you set up the environment needed to run a basic Apache Spark application using Amazon SageMaker Processing. By using Apache Spark on SageMaker Processing, you can run Spark jobs without having to provision an Amazon EMR cluster. You then define and run a Spark job using the **PySparkProcessor** class from the **SageMaker Python SDK**. Finally, you validate the data processing results saved in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "The processing script does some basic data processing, such as string indexing, one-hot encoding, vector assembly, and an 80-20 split of the processed data to train and validate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Setup the environment\n",
    "\n",
    "Install the latest SageMaker Python SDK package and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upgrading the SDK, restart your notebook kernel. \n",
    "\n",
    "1. Choose the **Restart kernel** icon from the notebook toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, import the required libraries, get the execution role to run the SageMaker processing job, and set up the Amazon S3 bucket to store the Spark job outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Execution Role:  arn:aws:iam::591057661366:role/LabVPC-notebook-role\n",
      "Bucket:  labdatabucket-us-west-2-736570222\n"
     ]
    }
   ],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, make sure you restarted your notebook kernel by selecting the **Restart kernel** icon from the notebook toolbar. Then, rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Run the SageMaker processing job\n",
    "\n",
    "In this task, you import and review the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>41</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>134724</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>3103</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>265168</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>278390</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>47</td>\n",
       "      <td>Private</td>\n",
       "      <td>652784</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>428420</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                  1       2              3   4                    5   \\\n",
       "958  41   Self-emp-not-inc  134724      Assoc-voc  11   Married-civ-spouse   \n",
       "284  33            Private  265168      Bachelors  13   Married-civ-spouse   \n",
       "370  23            Private  278390   Some-college  10   Married-civ-spouse   \n",
       "742  47            Private  652784           10th   6   Married-civ-spouse   \n",
       "130  41            Private  428420   Some-college  10   Married-civ-spouse   \n",
       "\n",
       "                     6         7       8        9     10  11  12  \\\n",
       "958       Other-service      Wife   White   Female  3103   0  40   \n",
       "284               Sales   Husband   Black     Male     0   0  55   \n",
       "370               Sales   Husband   White     Male     0   0  68   \n",
       "742   Machine-op-inspct   Husband   White     Male     0   0  40   \n",
       "130        Adm-clerical   Husband   White     Male     0   0  40   \n",
       "\n",
       "                 13      14  \n",
       "958   United-States    >50K  \n",
       "284   United-States   <=50K  \n",
       "370   United-States   <=50K  \n",
       "742   United-States   <=50K  \n",
       "130   United-States   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the SageMaker Spark PySparkProcessor class to define and run a spark application as a processing job. Refer to [SageMaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) for more information about this class.\n",
    "\n",
    "For creating the PySparkProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **framework_version**: SageMaker PySpark version\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance used for the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you use the PySparkProcessor run method to run the **pyspark_preprocessing.py** script as a processing job. Refer to [PySparkProcessor run method](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) for more information about this method. For this lab, data transformations such as string indexing and one-hot encoding are performed on the categorical features.\n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **submit_app**: Path of the preprocessing script \n",
    "- **outputs**: Path of output for the preprocessing script (Amazon S3 output locations)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as the Amazon S3 input and output locations)\n",
    "\n",
    "The processing job takes approximately 5 minutes to complete. While the job is running, you can review the source for the preprocessing script (which has been preconfigured as part of this lab) by opening the **pyspark_preprocessing.py** file from the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created S3 bucket: sagemaker-us-west-2-591057661366\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/18/25 01:26:38] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Created S3 bucket: sagemaker-us-west-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">591057661366</span>                     <a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#723\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/18/25 01:26:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Created S3 bucket: sagemaker-us-west-\u001b[1;36m2\u001b[0m-\u001b[1;36m591057661366\u001b[0m                     \u001b]8;id=768123;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=5489;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#723\u001b\\\u001b[2m723\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-preprocessor-2025-04-18-01-26-37-316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         sm-spark-preprocessor-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-18-01-26-37-316                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=36454;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=396375;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         sm-spark-preprocessor-\u001b[1;36m2025\u001b[0m-04-18-01-26-37-316                          \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\u001b[34m04-18 01:28 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     {'current_host': 'algo-2', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1', 'algo-2']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:591057661366:processing-job/sm-spark-preprocessor-2025-04-18-01-26-37-316', 'ProcessingJobName': 'sm-spark-preprocessor-2025-04-18-01-26-37-316', 'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-west-2-591057661366/sm-spark-preprocessor-2025-04-18-01-26-37-316/input/code/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::591057661366:role/LabVPC-notebook-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket labdatabucket-us-west-2-736570222 --s3_input_key_prefix data/input --s3_output_bucket labdatabucket-us-west-2-736570222 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34m04-18 01:28 waitress     INFO     Serving on http://10.0.168.83:5555\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.147.198</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.147.198\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[34m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[34mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34mspark.yarn.appMasterEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34mspark.executorEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,454 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-2/10.0.168.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//ha\u001b[0m\n",
      "\u001b[34mdoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,465 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,496 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-2/10.0.168.83\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//ha\u001b[0m\n",
      "\u001b[34mdoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,507 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,046 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,046 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,073 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,127 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,166 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,167 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,168 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,168 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,169 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,169 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,170 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,171 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,178 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,188 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,188 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,236 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,262 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,262 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,318 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,318 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,339 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,351 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,398 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@79e4c792\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,400 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,401 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,401 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,401 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,427 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,431 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,434 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@183ec003\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,435 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,436 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,436 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,436 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,436 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,436 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,445 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.3 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,446 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,475 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,477 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,478 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,478 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,482 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,483 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,485 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,485 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,487 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,487 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,504 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,506 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,506 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,512 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,551 INFO util.log: Logging initialized @1604ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,557 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,576 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,745 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,755 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,762 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,771 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,772 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,772 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,805 INFO http.HttpServer2: Jetty bound to port 36923\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,806 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,809 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,810 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,811 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,818 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:41685\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,824 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,824 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,829 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,830 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,830 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,831 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,833 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.168.83:41685\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,833 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.0.168.83:0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,833 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,835 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,835 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,836 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,836 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,847 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,848 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,861 INFO util.log: Logging initialized @1912ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,904 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,913 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,920 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:36923}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,920 INFO server.Server: Started @1974ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,953 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,959 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,965 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,966 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,966 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,966 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,966 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,966 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,967 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,968 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,968 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,050 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,058 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,058 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,066 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,108 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,124 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,358 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,369 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,372 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,373 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,375 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,384 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,394 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.147.198:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,401 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,402 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,409 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,409 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,410 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,418 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,420 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,421 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,430 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,534 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:23 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:24 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,105 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-1224143995444105723.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,111 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,111 INFO server.Server: Started @3163ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,112 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,115 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:41685\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,116 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,121 INFO client.RMProxy: Connecting to ResourceManager at /10.0.147.198:8031\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,173 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,184 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,482 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,236 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1', 'algo-2']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:591057661366:processing-job/sm-spark-preprocessor-2025-04-18-01-26-37-316', 'ProcessingJobName': 'sm-spark-preprocessor-2025-04-18-01-26-37-316', 'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'labdatabucket-us-west-2-736570222', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-736570222', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-west-2-591057661366/sm-spark-preprocessor-2025-04-18-01-26-37-316/input/code/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://labdatabucket-us-west-2-736570222/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::591057661366:role/LabVPC-notebook-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket labdatabucket-us-west-2-736570222 --s3_input_key_prefix data/input --s3_output_bucket labdatabucket-us-west-2-736570222 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34m04-18 01:28 waitress     INFO     Serving on http://10.0.147.198:5555\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.147.198</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.147.198\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[34m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[34mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34mspark.yarn.appMasterEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34mspark.executorEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m04-18 01:28 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,594 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,604 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:21,674 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,089 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,100 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,101 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,101 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,105 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,106 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,106 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,106 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,144 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,154 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,154 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,158 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,158 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Apr 18 01:28:22\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,160 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,160 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,161 INFO util.GSet: 2.0% max memory 3.1 GB = 63.0 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,161 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,186 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,186 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,192 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,192 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,192 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,192 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,193 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,212 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,212 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,212 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,212 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,223 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,223 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,223 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,223 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,233 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,234 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,234 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,234 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,238 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,240 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,244 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,244 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,244 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,244 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,251 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,251 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,251 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,255 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,255 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,256 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,256 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,257 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 968.3 KB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,257 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,281 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,297 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,319 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,397 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,406 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,409 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:22,410 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,485 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,503 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,549 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,586 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,618 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,634 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,776 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.147.198\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-\u001b[0m\n",
      "\u001b[34myarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:23,801 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,094 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,449 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,570 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,571 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,583 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,583 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,702 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,702 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,703 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,727 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,727 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,755 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,756 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,757 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,758 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,763 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.147.198/\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,767 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,768 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,768 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,776 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,781 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,798 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,799 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,802 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,819 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,874 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,919 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,952 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:24,967 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,019 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,019 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,019 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,020 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,049 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,061 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,071 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,076 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,076 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,088 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,115 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,126 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@79e4c792\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,128 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,130 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,130 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,130 INFO util.log: Logging initialized @2563ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,130 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,133 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,134 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,136 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,137 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,200 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,205 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,218 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@183ec003\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,218 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,219 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,219 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,219 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,219 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,219 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,231 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.3 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,231 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,238 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,293 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,294 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,305 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,310 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,312 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,329 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,338 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,366 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,379 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,379 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,381 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,382 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,385 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,387 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,391 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,404 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:25,483 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:26,237 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,417 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,419 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,422 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,422 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,422 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,433 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,436 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,436 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,443 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,451 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,453 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,453 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,460 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,463 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,466 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,470 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,491 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,492 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,492 INFO util.log: Logging initialized @2926ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,509 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,509 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,520 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,532 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,595 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,596 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,632 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,633 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,666 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,666 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,671 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,673 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,674 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,675 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,675 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,676 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,677 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,677 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,678 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,679 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,682 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,688 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@302c971f{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,689 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61c4eee0{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,696 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,696 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,699 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,706 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,772 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,774 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,783 INFO util.log: Logging initialized @3211ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,794 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,804 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,806 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,806 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,807 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,812 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4a00d9cf{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,837 INFO server.AbstractConnector: Started ServerConnector@50378a4{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,837 INFO server.Server: Started @3271ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,848 INFO http.HttpServer2: Jetty bound to port 46513\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,851 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,887 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,887 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,889 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,893 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,894 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,901 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,901 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,902 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,921 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,947 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,955 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,963 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,963 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,964 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,964 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,965 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,965 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,967 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,967 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,967 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,990 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:25,993 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:37721\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,001 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,004 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,006 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,009 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,011 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:46513}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,011 INFO server.Server: Started @3445ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,012 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,013 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,014 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,056 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.147.198:37721\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,056 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.147.198:0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,056 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,063 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,120 INFO util.log: Logging initialized @3541ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,305 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,310 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,317 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,318 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,318 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,319 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,319 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,320 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,320 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,322 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,322 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,334 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,334 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,396 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,416 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,417 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,417 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,459 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,490 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,491 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,492 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,498 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,500 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,500 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,500 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,500 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,541 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,616 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,629 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,629 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,634 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,637 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Apr 18 01:28:26\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,639 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,639 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,649 INFO util.GSet: 2.0% max memory 3.1 GB = 63.0 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,650 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,668 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,669 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,678 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,678 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,679 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,679 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,680 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,744 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,744 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,745 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,745 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,763 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,764 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,764 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,764 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,777 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,778 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,778 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,778 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,785 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,787 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,793 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,793 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,794 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,794 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,808 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,813 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,813 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,816 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,816 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,825 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 968.3 KB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,825 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:26,484 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:27,238 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,862 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 115@algo-1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,916 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,922 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,923 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,924 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,924 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:26,925 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,013 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,014 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,016 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,030 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,074 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,091 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,095 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,096 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,121 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,125 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@352c1b98{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,126 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,134 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,139 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,152 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.147.198:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,188 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,204 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,223 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,227 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,228 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,284 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,320 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,320 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,326 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,331 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,338 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,339 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,341 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,351 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,354 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,355 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,371 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,429 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,544 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,573 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,573 INFO namenode.FSNamesystem: Finished loading FSImage in 741 msecs\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:27 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,905 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:27,978 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,010 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:28 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:27,484 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:28,239 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:28,485 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,331 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:28 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,542 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,551 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,587 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,652 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,664 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,664 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,664 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,786 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,768 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,868 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.147.198:8020\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,903 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 239 msec\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,921 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,922 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:28 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:28,970 INFO namenode.FSDirectory: Quota initialization completed in 48 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,027 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,047 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-1-8042-_-any-7751661181287186166.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,063 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,063 INFO server.Server: Started @6485ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,064 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,065 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:37721\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,091 INFO client.RMProxy: Connecting to ResourceManager at /10.0.147.198:8031\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,141 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34mApr 18, 2025 1:28:29 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,315 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,343 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,239 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,365 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.147.198:8020\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,367 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,372 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 17@algo-2\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,373 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 544415134. Formatting...\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,374 INFO common.Storage: Generated new storageID DS-0c793578-4472-4c4c-893e-90c28b56bccd for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,394 INFO common.Storage: Analyzing storage directories for bpid BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,394 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,394 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1149909736-10.0.147.198-1744939702274 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,394 INFO common.Storage: Formatting block pool BP-1149909736-10.0.147.198-1744939702274 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,399 INFO datanode.DataNode: Setting up storage: nsid=544415134;bpid=BP-1149909736-10.0.147.198-1744939702274;lv=-57;nsInfo=lv=-65;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274;bpid=BP-1149909736-10.0.147.198-1744939702274;dnuuid=null\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,400 INFO datanode.DataNode: Generated and persisted new Datanode UUID ab676c69-fc27-4373-b2dd-dc406987d737\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,478 INFO impl.FsDatasetImpl: Added new volume: DS-0c793578-4472-4c4c-893e-90c28b56bccd\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,478 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,482 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,487 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,493 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,495 INFO impl.FsDatasetImpl: Adding block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,495 INFO impl.FsDatasetImpl: Scanning block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,522 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1149909736-10.0.147.198-1744939702274 on /opt/amazon/hadoop/hdfs/datanode: 27ms\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,522 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1149909736-10.0.147.198-1744939702274: 28ms\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,524 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,524 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,525 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode: 2ms\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,526 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274: 2ms\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,527 INFO datanode.VolumeScanner: Now scanning bpid BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,529 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-0c793578-4472-4c4c-893e-90c28b56bccd): finished scanning block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,542 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-0c793578-4472-4c4c-893e-90c28b56bccd): no suitable block pools found to scan.  Waiting 1814399985 ms.\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,543 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/18/25 5:35 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,548 INFO datanode.DataNode: Block pool BP-1149909736-10.0.147.198-1744939702274 (Datanode Uuid ab676c69-fc27-4373-b2dd-dc406987d737) service to algo-1/10.0.147.198:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,642 INFO datanode.DataNode: Block pool Block pool BP-1149909736-10.0.147.198-1744939702274 (Datanode Uuid ab676c69-fc27-4373-b2dd-dc406987d737) service to algo-1/10.0.147.198:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,642 INFO datanode.DataNode: For namenode algo-1/10.0.147.198:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,946 INFO datanode.DataNode: Successfully sent block report 0x795f442f558d2251,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 115 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:29,947 INFO datanode.DataNode: Got finalize command for block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:30,240 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,384 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@499683c4{cluster,/,file:///tmp/jetty-10_0_147_198-8088-_-any-3571931696022854858.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,396 INFO server.AbstractConnector: Started ServerConnector@51549490{HTTP/1.1,[http/1.1]}{10.0.147.198:8088}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,397 INFO server.Server: Started @6824ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,404 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,467 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.147.198:8020\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,470 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,486 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,489 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 116@algo-1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,491 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 544415134. Formatting...\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,492 INFO common.Storage: Generated new storageID DS-13996b60-3d3e-4418-9908-3e157b210fc1 for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,497 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,577 INFO common.Storage: Analyzing storage directories for bpid BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,580 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,582 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1149909736-10.0.147.198-1744939702274 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,582 INFO common.Storage: Formatting block pool BP-1149909736-10.0.147.198-1744939702274 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,589 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.168.83:9866, datanodeUuid=ab676c69-fc27-4373-b2dd-dc406987d737, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274) storage ab676c69-fc27-4373-b2dd-dc406987d737\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,591 INFO datanode.DataNode: Setting up storage: nsid=544415134;bpid=BP-1149909736-10.0.147.198-1744939702274;lv=-57;nsInfo=lv=-65;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274;bpid=BP-1149909736-10.0.147.198-1744939702274;dnuuid=null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,594 INFO datanode.DataNode: Generated and persisted new Datanode UUID da439782-f2fe-4eda-a60a-1a4e3ca71559\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,618 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.168.83:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,618 INFO blockmanagement.BlockReportLeaseManager: Registered DN ab676c69-fc27-4373-b2dd-dc406987d737 (10.0.168.83:9866).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,769 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c793578-4472-4c4c-893e-90c28b56bccd for DN 10.0.168.83:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,850 INFO BlockStateChange: BLOCK* processReport 0x795f442f558d2251: Processing first storage report for DS-0c793578-4472-4c4c-893e-90c28b56bccd from datanode ab676c69-fc27-4373-b2dd-dc406987d737\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,853 INFO BlockStateChange: BLOCK* processReport 0x795f442f558d2251: from storage DS-0c793578-4472-4c4c-893e-90c28b56bccd node DatanodeRegistration(10.0.168.83:9866, datanodeUuid=ab676c69-fc27-4373-b2dd-dc406987d737, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,931 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,958 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,933 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,970 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,981 INFO impl.FsDatasetImpl: Added new volume: DS-13996b60-3d3e-4418-9908-3e157b210fc1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,981 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:29,988 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m04-18 01:28 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m04-18 01:28 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-18 01:28 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m04-18 01:28 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2025-04-18T01:28:29.995696')), ('algo-2', StatusMessage(status='WAITING', timestamp='2025-04-18T01:28:29.999438'))])\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,004 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,009 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,015 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,018 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,018 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,018 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,018 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,027 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,028 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,028 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,028 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,029 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,031 INFO impl.FsDatasetImpl: Adding block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,032 INFO impl.FsDatasetImpl: Scanning block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,056 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,099 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1149909736-10.0.147.198-1744939702274 on /opt/amazon/hadoop/hdfs/datanode: 68ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,099 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1149909736-10.0.147.198-1744939702274: 68ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,101 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,102 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,128 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode: 27ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,128 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1149909736-10.0.147.198-1744939702274: 28ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,134 INFO datanode.VolumeScanner: Now scanning bpid BP-1149909736-10.0.147.198-1744939702274 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,136 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-13996b60-3d3e-4418-9908-3e157b210fc1): finished scanning block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,170 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-13996b60-3d3e-4418-9908-3e157b210fc1): no suitable block pools found to scan.  Waiting 1814399960 ms.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,172 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/18/25 2:44 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,190 INFO datanode.DataNode: Block pool BP-1149909736-10.0.147.198-1744939702274 (Datanode Uuid da439782-f2fe-4eda-a60a-1a4e3ca71559) service to algo-1/10.0.147.198:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,245 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.147.198:9866, datanodeUuid=da439782-f2fe-4eda-a60a-1a4e3ca71559, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274) storage da439782-f2fe-4eda-a60a-1a4e3ca71559\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,246 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,246 INFO blockmanagement.BlockReportLeaseManager: Registered DN da439782-f2fe-4eda-a60a-1a4e3ca71559 (10.0.147.198:9866).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,249 INFO datanode.DataNode: Block pool Block pool BP-1149909736-10.0.147.198-1744939702274 (Datanode Uuid da439782-f2fe-4eda-a60a-1a4e3ca71559) service to algo-1/10.0.147.198:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,251 INFO datanode.DataNode: For namenode algo-1/10.0.147.198:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,306 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-13996b60-3d3e-4418-9908-3e157b210fc1 for DN 10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,334 INFO BlockStateChange: BLOCK* processReport 0xac58ac2297107f37: Processing first storage report for DS-13996b60-3d3e-4418-9908-3e157b210fc1 from datanode da439782-f2fe-4eda-a60a-1a4e3ca71559\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,334 INFO BlockStateChange: BLOCK* processReport 0xac58ac2297107f37: from storage DS-13996b60-3d3e-4418-9908-3e157b210fc1 node DatanodeRegistration(10.0.147.198:9866, datanodeUuid=da439782-f2fe-4eda-a60a-1a4e3ca71559, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-884e26a1-3be5-4ceb-8ce9-cda8e549d357;nsid=544415134;c=1744939702274), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,419 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,453 INFO datanode.DataNode: Successfully sent block report 0xac58ac2297107f37,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 131 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,453 INFO datanode.DataNode: Got finalize command for block pool BP-1149909736-10.0.147.198-1744939702274\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:31,241 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,471 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,491 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,491 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,519 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,519 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,534 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,535 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,544 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,546 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,546 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,644 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,646 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,653 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,693 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,697 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,698 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,812 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,813 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,825 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,825 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,828 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:30,863 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,423 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,454 INFO rmnode.RMNodeImpl: algo-1:37721 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,454 INFO rmnode.RMNodeImpl: algo-2:41685 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,455 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 41685 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:41685\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,455 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 37721 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:37721\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:31,496 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -701920898\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:31,496 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 280176094\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:31,497 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:41685 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,485 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -701920898\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,487 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 280176094\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,488 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:37721 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,499 INFO capacity.CapacityScheduler: Added node algo-1:37721 clusterResource: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:31,504 INFO capacity.CapacityScheduler: Added node algo-2:41685 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,505 INFO spark.SparkContext: Running Spark version 3.1.1-amzn-0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,550 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,550 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,550 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,551 INFO spark.SparkContext: Submitted application: PySparkApp\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,569 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,584 INFO resource.ResourceProfile: Limiting resource is cpus at 4 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,586 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,632 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,632 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,632 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,632 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,632 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,883 INFO util.Utils: Successfully started service 'sparkDriver' on port 41025.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,911 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,942 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,967 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,967 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:33,998 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,011 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-537cedf1-3b33-437d-b03e-e7c8e528bf8c\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,032 INFO memory.MemoryStore: MemoryStore started with capacity 1028.8 MiB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,075 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,180 INFO util.log: Logging initialized @3556ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,249 INFO server.Server: jetty-9.4.37.v20210219; built: 2021-02-19T15:16:47.689Z; git: 27afab2bd37780d179836e313e0fe11bc4fa0ce9; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,270 INFO server.Server: Started @3647ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,303 INFO server.AbstractConnector: Started ServerConnector@50afd9a7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,303 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,323 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a4252ea{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,325 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e3468f8{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,326 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b3bd9ff{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,327 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6966463c{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,328 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46bae335{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,328 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cb61344{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,329 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4db5260c{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,331 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77b50041{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,332 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@befc7fb{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,332 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@616df827{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,333 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@91c6814{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,334 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13aab67f{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,334 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48575e7d{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,335 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55125287{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,335 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bfe7510{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,336 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37379aac{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,337 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75ccc9e5{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,338 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d527d12{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,339 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@306328f5{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,339 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73bdbe5b{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,347 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@638ebcb0{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,348 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6426ba51{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,349 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c9c34ea{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,350 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69218fa2{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,351 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c465f5b{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,353 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.147.198:4040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:34,683 INFO client.RMProxy: Connecting to ResourceManager at /10.0.147.198:8032\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,028 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,050 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,491 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,492 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,508 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,509 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,509 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,510 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,515 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:35,591 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,007 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001 src: /10.0.147.198:44926 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:41,713 INFO yarn.Client: Uploading resource file:/tmp/spark-f94a86ad-15a5-49aa-88de-f1448c071264/__spark_libs__88693326870751766.zip -> hdfs://10.0.147.198/user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:41,824 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:41,945 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001 src: /10.0.147.198:37616 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,526 INFO DataNode.clienttrace: src: /10.0.147.198:44926, dest: /10.0.168.83:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001, duration(ns): 495655213\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,526 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,542 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002 src: /10.0.147.198:44932 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,980 INFO DataNode.clienttrace: src: /10.0.147.198:44932, dest: /10.0.168.83:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002, duration(ns): 436319717\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,980 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:42,989 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003 src: /10.0.147.198:44946 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,530 INFO DataNode.clienttrace: src: /10.0.147.198:37616, dest: /10.0.147.198:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001, duration(ns): 494790796\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,531 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,536 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,540 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002 src: /10.0.147.198:37622 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,981 INFO DataNode.clienttrace: src: /10.0.147.198:37622, dest: /10.0.147.198:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002, duration(ns): 437402287\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,982 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,984 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:42,987 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003 src: /10.0.147.198:37624 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,355 INFO DataNode.clienttrace: src: /10.0.147.198:37624, dest: /10.0.147.198:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003, duration(ns): 364686043\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,356 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,359 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,363 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004 src: /10.0.147.198:37638 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,470 INFO DataNode.clienttrace: src: /10.0.147.198:37638, dest: /10.0.147.198:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004, duration(ns): 103487325\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,470 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,354 INFO DataNode.clienttrace: src: /10.0.147.198:44946, dest: /10.0.168.83:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003, duration(ns): 363440715\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,354 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,364 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004 src: /10.0.147.198:44958 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,468 INFO DataNode.clienttrace: src: /10.0.147.198:44958, dest: /10.0.168.83:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004, duration(ns): 102290015\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,468 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,594 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005 src: /10.0.147.198:44962 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,601 INFO DataNode.clienttrace: src: /10.0.147.198:44962, dest: /10.0.168.83:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005, duration(ns): 5176792\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,601 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,623 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006 src: /10.0.147.198:44974 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,627 INFO DataNode.clienttrace: src: /10.0.147.198:44974, dest: /10.0.168.83:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006, duration(ns): 1963304\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:43,627 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:44,162 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007 src: /10.0.147.198:44976 dest: /10.0.168.83:9866\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:44,166 INFO DataNode.clienttrace: src: /10.0.147.198:44976, dest: /10.0.168.83:9866, bytes: 266131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: ab676c69-fc27-4373-b2dd-dc406987d737, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007, duration(ns): 2846299\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:44,166 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,477 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip is closed by DFSClient_NONMAPREDUCE_1280434397_18\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,581 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.147.198/user/root/.sparkStaging/application_1744939709972_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,589 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,593 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005 src: /10.0.147.198:37650 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,603 INFO DataNode.clienttrace: src: /10.0.147.198:37650, dest: /10.0.147.198:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005, duration(ns): 6019647\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,603 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,604 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1744939709972_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_1280434397_18\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,613 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://10.0.147.198/user/root/.sparkStaging/application_1744939709972_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,619 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,622 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006 src: /10.0.147.198:37662 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,627 INFO DataNode.clienttrace: src: /10.0.147.198:37662, dest: /10.0.147.198:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006, duration(ns): 2795276\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,628 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:43,629 INFO namenode.FSNamesystem: BLOCK* blk_1073741830_1006 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1744939709972_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,031 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1744939709972_0001/py4j-0.10.9-src.zip is closed by DFSClient_NONMAPREDUCE_1280434397_18\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,150 INFO yarn.Client: Uploading resource file:/tmp/spark-f94a86ad-15a5-49aa-88de-f1448c071264/__spark_conf__6048304532302554018.zip -> hdfs://10.0.147.198/user/root/.sparkStaging/application_1744939709972_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,157 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.147.198:9866, 10.0.168.83:9866 for /user/root/.sparkStaging/application_1744939709972_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,160 INFO datanode.DataNode: Receiving BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007 src: /10.0.147.198:37678 dest: /10.0.147.198:9866\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,167 INFO DataNode.clienttrace: src: /10.0.147.198:37678, dest: /10.0.147.198:9866, bytes: 266131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1280434397_18, offset: 0, srvID: da439782-f2fe-4eda-a60a-1a4e3ca71559, blockid: BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007, duration(ns): 3629195\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,168 INFO datanode.DataNode: PacketResponder: BP-1149909736-10.0.147.198-1744939702274:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.168.83:9866] terminating\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,169 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1744939709972_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1280434397_18\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,189 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,190 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,190 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,190 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,190 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,212 INFO yarn.Client: Submitting application application_1744939709972_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,293 INFO capacity.CapacityScheduler: Application 'application_1744939709972_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,293 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,309 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,311 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,311 INFO rmapp.RMAppImpl: Storing application with id application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,313 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.147.198#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,320 INFO recovery.RMStateStore: Storing info for app: application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,320 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,321 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,322 INFO capacity.ParentQueue: Application added - appId: application_1744939709972_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,323 INFO capacity.CapacityScheduler: Accepted application application_1744939709972_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,335 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,365 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,366 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,421 INFO impl.YarnClientImpl: Submitted application application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,429 INFO capacity.LeafQueue: Application application_1744939709972_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,429 INFO capacity.LeafQueue: Application added - appId: application_1744939709972_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,429 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1744939709972_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,436 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:44,877 INFO ipc.Server: Auth successful for appattempt_1744939709972_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:44,962 INFO containermanager.ContainerManagerImpl: Start request for container_1744939709972_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,014 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,023 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,023 INFO application.ApplicationImpl: Adding container_1744939709972_0001_01_000001 to application application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,024 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.147.198#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,028 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,031 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,031 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,043 INFO localizer.ResourceLocalizationService: Created localizer for container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,117 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1744939709972_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,133 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,148 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1744939709972_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:45,148 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,624 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1744939709972_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,628 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,640 INFO fica.FiCaSchedulerNode: Assigned container container_1744939709972_0001_01_000001 of capacity <memory:896, max memory:15892, vCores:1, max vCores:4> on host algo-2:41685, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,640 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,665 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:41685 for container : container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,674 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,674 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,674 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,675 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,675 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1744939709972_0001 AttemptId: appattempt_1744939709972_0001_000001 MasterContainer: Container: [ContainerId: container_1744939709972_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:41685, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.168.83:41685 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,685 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,688 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,700 INFO amlauncher.AMLauncher: Launching masterappattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,753 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1744939709972_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:41685, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.168.83:41685 }, ExecutionType: GUARANTEED, ] for AM appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,754 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:44,756 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,038 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1744939709972_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:41685, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.168.83:41685 }, ExecutionType: GUARANTEED, ] for AM appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,038 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,039 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1744939709972_0001, attemptId: appattempt_1744939709972_0001_000001launchTime: 1744939725038\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,039 INFO recovery.RMStateStore: Updating info for app: application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,428 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,430 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1744939724309\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1744939709972_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:45,620 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:46,433 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:47,435 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:48,438 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:48,580 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:48,581 INFO scheduler.ContainerScheduler: Starting container [container_1744939709972_0001_01_000001]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:48,611 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:48,612 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:48,615 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:49,441 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:49,840 INFO monitor.ContainersMonitorImpl: container_1744939709972_0001_01_000001's ip = 10.0.168.83, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:49,847 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1744939709972_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m04-18 01:28 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,444 INFO yarn.Client: Application report for application_1744939709972_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,580 INFO ipc.Server: Auth successful for appattempt_1744939709972_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,600 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,600 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.168.83#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011APPATTEMPTID=appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,600 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,601 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:50,809 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1744939709972_0001), /proxy/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,446 INFO yarn.Client: Application report for application_1744939709972_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,447 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.168.83\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1744939724309\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1744939709972_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,448 INFO cluster.YarnClientSchedulerBackend: Application application_1744939709972_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,456 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33175.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,456 INFO netty.NettyBlockTransferService: Server created on 10.0.147.198:33175\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,458 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,469 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.147.198, 33175, None)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,475 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.147.198:33175 with 1028.8 MiB RAM, BlockManagerId(driver, 10.0.147.198, 33175, None)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,478 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.147.198, 33175, None)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,479 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.147.198, 33175, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,523 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,524 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,524 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,944 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,944 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,945 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,945 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:49,945 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,044 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,230 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,451 INFO client.RMProxy: Connecting to ResourceManager at /10.0.147.198:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,500 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,711 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:41025 after 55 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:50,824 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:51,459 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     AWS_REGION -> us-west-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.147.198/user/root/.sparkStaging/application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       -Xmx12399m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-Dspark.driver.port=41025' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.147.198:41025 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       4 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       application_1744939709972_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.147.198\" port: -1 file: \"/user/root/.sparkStaging/application_1744939709972_0001/pyspark.zip\" } size: 889814 timestamp: 1744939723604 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.147.198\" port: -1 file: \"/user/root/.sparkStaging/application_1744939709972_0001/__spark_libs__88693326870751766.zip\" } size: 448655305 timestamp: 1744939723477 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.147.198\" port: -1 file: \"/user/root/.sparkStaging/application_1744939709972_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1744939724031 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.147.198\" port: -1 file: \"/user/root/.sparkStaging/application_1744939709972_0001/__spark_conf__.zip\" } size: 266131 timestamp: 1744939724169 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:51,529 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stderr] 2025-04-18 01:28:51,530 INFO yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,660 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,715 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,717 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1846f2e2{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:51,754 INFO history.SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/application_1744939709972_0001.inprogress\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,016 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,261 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,261 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,274 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,276 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68f5f4f0{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,276 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,277 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41db8d6e{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,278 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,279 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@687967b{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,279 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,280 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fcf96d{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,281 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,282 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20cd61d6{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,625 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1744939709972_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,625 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,626 INFO fica.FiCaSchedulerNode: Assigned container container_1744939709972_0001_01_000002 of capacity <memory:13638, vCores:1> on host algo-1:37721, which has 1 containers, <memory:13638, vCores:1> used and <memory:2254, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,626 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,626 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,626 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,642 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1744939709972_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,642 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,642 INFO fica.FiCaSchedulerNode: Assigned container container_1744939709972_0001_01_000003 of capacity <memory:13638, vCores:1> on host algo-2:41685, which has 2 containers, <memory:14534, vCores:2> used and <memory:1358, vCores:2> available after allocation\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,642 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,643 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8863579 absoluteUsedCapacity=0.8863579 used=<memory:28172, vCores:3> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:52,643 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,050 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,061 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,061 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,277 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:37721 for container : container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,278 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,280 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:41685 for container : container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,281 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,390 INFO ipc.Server: Auth successful for appattempt_1744939709972_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,395 INFO containermanager.ContainerManagerImpl: Start request for container_1744939709972_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,397 INFO application.ApplicationImpl: Adding container_1744939709972_0001_01_000003 to application application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,397 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.168.83#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,398 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,399 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,399 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,400 INFO scheduler.ContainerScheduler: Starting container [container_1744939709972_0001_01_000003]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,418 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,418 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:53,423 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000001/stdHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,500 INFO ipc.Server: Auth successful for appattempt_1744939709972_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,642 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,643 INFO containermanager.ContainerManagerImpl: Start request for container_1744939709972_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,705 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,715 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.168.83#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,715 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,715 INFO application.ApplicationImpl: Adding container_1744939709972_0001_01_000002 to application application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,721 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,728 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,728 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,741 INFO localizer.ResourceLocalizationService: Created localizer for container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,827 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1744939709972_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,844 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,851 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1744939709972_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:53,851 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:54,619 INFO datasources.InMemoryFileIndex: It took 84 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:54,667 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:55,876 INFO monitor.ContainersMonitorImpl: container_1744939709972_0001_01_000003's ip = 10.0.168.83, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m2025-04-18 01:28:55,885 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1744939709972_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:56,044 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.168.83:39498) with ID 2,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:56,237 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:37291 with 6.3 GiB RAM, BlockManagerId(2, algo-2, 37291, None)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:56,308 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,370 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 508@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,379 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,380 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,380 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,909 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,910 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,911 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,911 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:54,912 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,270 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:41025 after 63 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,404 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,404 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,404 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,404 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,404 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,453 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:41025 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,518 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/blockmgr-c2104e38-eb0e-4805-9cab-59900f07d3d7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,559 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,308 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,310 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,313 INFO datasources.FileSourceStrategy: Output Data Schema: struct<workclass: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,915 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.5 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,993 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:57,996 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,003 INFO spark.SparkContext: Created broadcast 0 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,077 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,084 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,145 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at StringIndexer.scala:204) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,149 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,150 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,150 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,152 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,158 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,414 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,421 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,422 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,424 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,448 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,449 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,514 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,690 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,691 INFO scheduler.ContainerScheduler: Starting container [container_1744939709972_0001_01_000002]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,718 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,719 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,724 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:58,798 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:37291 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:59,061 INFO monitor.ContainersMonitorImpl: container_1744939709972_0001_01_000002's ip = 10.0.147.198, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:59,067 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1744939709972_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,878 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.147.198:41025\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,896 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,896 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:55,897 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,062 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,067 INFO executor.Executor: Starting executor ID 2 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,216 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37291.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,216 INFO netty.NettyBlockTransferService: Server created on algo-2:37291\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,218 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,220 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,227 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 37291, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,241 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 37291, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,241 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 37291, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,245 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:56,245 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:57,145 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1182 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,548 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,562 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,685 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:28:59,970 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:37291 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:01,285 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.147.198:48300) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:01,492 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:33579 with 6.3 GiB RAM, BlockManagerId(1, algo-1, 33579, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:28:59,678 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1224@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:28:59,686 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:28:59,687 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:28:59,687 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,209 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,210 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,211 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,211 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,212 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,609 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:41025 after 80 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,706 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,706 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,706 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,707 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,707 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,768 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:41025 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,827 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/blockmgr-cb40885c-aa24-4c61-98e2-d1c8a83e8422\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:00,874 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,276 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3788 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,279 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,306 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at StringIndexer.scala:204) finished in 4.126 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,309 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,309 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,310 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,310 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,528 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,538 INFO scheduler.DAGScheduler: Got job 1 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,539 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,539 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,540 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,544 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,558 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,560 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,561 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,566 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,568 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,568 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,578 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,603 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:37291 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,767 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.168.83:39498\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,918 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 343 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,919 INFO scheduler.DAGScheduler: ResultStage 2 (collect at StringIndexer.scala:204) finished in 0.367 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,922 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,922 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,923 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:02,924 INFO scheduler.DAGScheduler: Job 1 finished: collect at StringIndexer.scala:204, took 0.395598 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,116 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,134 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:37291 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,175 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:37291 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,192 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,234 INFO codegen.CodeGenerator: Code generated in 228.144853 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,748 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:33175 after 3 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,792 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,803 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 117 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:58,888 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:59,524 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:59,958 INFO codegen.CodeGenerator: Code generated in 228.57643 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:59,959 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:59,968 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:28:59,972 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:00,027 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:00,623 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:00,671 INFO codegen.CodeGenerator: Code generated in 8.269956 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:00,693 INFO codegen.CodeGenerator: Code generated in 9.135734 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:00,712 INFO codegen.CodeGenerator: Code generated in 10.929317 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:01,746 INFO codegen.CodeGenerator: Code generated in 14.32543 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:01,918 INFO codegen.CodeGenerator: Code generated in 21.059614 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,259 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2119 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,583 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,583 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,713 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,713 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,713 INFO datasources.FileSourceStrategy: Output Data Schema: struct<education: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,746 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,758 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,758 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,759 INFO spark.SparkContext: Created broadcast 3 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,760 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,761 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,765 INFO scheduler.DAGScheduler: Registering RDD 10 (collect at StringIndexer.scala:204) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,766 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,766 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,766 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,766 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,766 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,774 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,775 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,776 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,776 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,777 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,777 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,779 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,797 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:37291 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,815 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:37291 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,924 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 145 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,924 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,926 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (collect at StringIndexer.scala:204) finished in 0.157 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,926 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,926 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,926 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,926 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,956 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,958 INFO scheduler.DAGScheduler: Got job 3 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,958 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,958 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,958 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,959 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,961 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\u001b[0m\n",
      "\u001b[34mjava.util.ConcurrentModificationException\u001b[0m\n",
      "\u001b[34m#011at java.util.Hashtable$Enumerator.next(Hashtable.java:1408)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.Iterator.foreach(Iterator.scala:941)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.Iterator.foreach$(Iterator.scala:941)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.IterableLike.foreach(IterableLike.scala:74)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\u001b[0m\n",
      "\u001b[34m#011at scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\u001b[0m\n",
      "\u001b[34m#011at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\u001b[0m\n",
      "\u001b[34m#011at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\u001b[0m\n",
      "\u001b[34m#011at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,963 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,965 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,966 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,967 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,967 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,967 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,969 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,982 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:37291 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:03,989 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.168.83:39498\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,025 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 56 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,026 INFO scheduler.DAGScheduler: ResultStage 5 (collect at StringIndexer.scala:204) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,028 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,028 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,029 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,029 INFO scheduler.DAGScheduler: Job 3 finished: collect at StringIndexer.scala:204, took 0.072635 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,151 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,151 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,152 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marital_status: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,173 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,182 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,183 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,184 INFO spark.SparkContext: Created broadcast 6 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,185 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,185 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,190 INFO scheduler.DAGScheduler: Registering RDD 17 (collect at StringIndexer.scala:204) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,190 INFO scheduler.DAGScheduler: Got map stage job 4 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,190 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,190 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,190 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,191 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,200 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,202 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,202 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,203 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,203 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,203 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,205 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:04,432 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:33579 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,133 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.147.198:41025\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,149 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,149 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,150 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,292 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,297 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,468 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33579.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,469 INFO netty.NettyBlockTransferService: Server created on algo-1:33579\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,471 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,485 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 33579, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,495 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 33579, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,496 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 33579, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,516 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,549 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:01,549 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:02,543 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1330 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,213 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,223 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,278 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:05,581 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,591 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,592 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,601 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,604 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 12 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,605 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,759 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,761 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,806 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,830 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,831 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,865 INFO codegen.CodeGenerator: Code generated in 11.705847 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:02,909 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 1). 3530 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,782 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,782 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,785 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,795 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,798 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,799 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,806 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,808 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,814 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,816 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,828 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,875 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,920 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,971 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,972 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,973 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,975 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,980 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,983 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,984 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,988 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,988 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,990 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,992 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,637 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 3433 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,637 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,639 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at StringIndexer.scala:204) finished in 3.447 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,639 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,639 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,639 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,639 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,667 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,669 INFO scheduler.DAGScheduler: Got job 5 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,669 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,669 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,670 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,670 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,673 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,682 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,692 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:33579 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,695 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,695 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,696 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,696 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,698 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,710 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,728 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:33579 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,739 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:37291 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,749 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,341 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,395 INFO client.TransportClientFactory: Successfully created connection to /10.0.147.198:33175 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,428 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,436 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 95 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:04,511 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,121 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,571 INFO codegen.CodeGenerator: Code generated in 198.275767 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,573 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,579 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,582 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:05,639 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:06,156 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:06,195 INFO codegen.CodeGenerator: Code generated in 7.795399 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:06,212 INFO codegen.CodeGenerator: Code generated in 6.51469 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:06,226 INFO codegen.CodeGenerator: Code generated in 8.110855 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,251 INFO codegen.CodeGenerator: Code generated in 15.093211 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,387 INFO codegen.CodeGenerator: Code generated in 17.493429 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,634 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 4). 2162 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,700 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,808 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:37291 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,808 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,828 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:37291 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,838 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:07,898 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.147.198:48300\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,045 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 347 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,045 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,046 INFO scheduler.DAGScheduler: ResultStage 8 (collect at StringIndexer.scala:204) finished in 0.375 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,046 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,048 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,048 INFO scheduler.DAGScheduler: Job 5 finished: collect at StringIndexer.scala:204, took 0.380502 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,156 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,156 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,156 INFO datasources.FileSourceStrategy: Output Data Schema: struct<occupation: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,182 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,194 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,195 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,197 INFO spark.SparkContext: Created broadcast 9 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,198 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,198 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,203 INFO scheduler.DAGScheduler: Registering RDD 24 (collect at StringIndexer.scala:204) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,203 INFO scheduler.DAGScheduler: Got map stage job 6 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,203 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,203 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,203 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,204 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,213 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,214 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,215 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,215 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,216 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,216 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,217 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,233 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:33579 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,246 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,344 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 127 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,344 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,345 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at StringIndexer.scala:204) finished in 0.140 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,346 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,347 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,347 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,347 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,371 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,373 INFO scheduler.DAGScheduler: Got job 7 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,373 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,373 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,374 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,375 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,378 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,380 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,381 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,382 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,382 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,382 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,384 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,400 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:33579 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,406 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.147.198:48300\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,445 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 62 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,445 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,447 INFO scheduler.DAGScheduler: ResultStage 11 (collect at StringIndexer.scala:204) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,447 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,447 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,448 INFO scheduler.DAGScheduler: Job 7 finished: collect at StringIndexer.scala:204, took 0.076544 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,543 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,543 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,543 INFO datasources.FileSourceStrategy: Output Data Schema: struct<relationship: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,561 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 317.5 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,570 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,571 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,572 INFO spark.SparkContext: Created broadcast 12 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,574 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,574 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,577 INFO scheduler.DAGScheduler: Registering RDD 31 (collect at StringIndexer.scala:204) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,578 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,578 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,578 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,578 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,578 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,586 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,592 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,595 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,595 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,596 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,596 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,597 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,621 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:33579 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,637 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,643 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:33579 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,657 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,657 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:33579 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,665 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,671 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:33579 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,681 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,687 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,697 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:33579 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,713 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:33579 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,717 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,708 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,717 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,719 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,726 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,729 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,731 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,893 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,894 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,920 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,947 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,948 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:07,993 INFO codegen.CodeGenerator: Code generated in 14.657303 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,041 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 5). 3537 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,220 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,220 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,223 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,230 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,233 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,235 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,239 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,241 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,245 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,247 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,263 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,293 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,341 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 6). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,386 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,386 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,388 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,389 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,398 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,401 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,402 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,405 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,405 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,408 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,759 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 162 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,760 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,761 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (collect at StringIndexer.scala:204) finished in 0.180 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,762 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,762 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,762 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,762 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,790 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,792 INFO scheduler.DAGScheduler: Got job 9 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,792 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,792 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,793 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,794 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,796 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,798 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,799 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,799 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,800 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,800 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,801 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,815 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:33579 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,823 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.147.198:48300\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,870 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,870 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,871 INFO scheduler.DAGScheduler: ResultStage 14 (collect at StringIndexer.scala:204) finished in 0.076 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,873 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,873 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,874 INFO scheduler.DAGScheduler: Job 9 finished: collect at StringIndexer.scala:204, took 0.083035 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,977 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,978 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,978 INFO datasources.FileSourceStrategy: Output Data Schema: struct<race: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:08,999 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,011 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,012 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,013 INFO spark.SparkContext: Created broadcast 15 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,014 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,014 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,019 INFO scheduler.DAGScheduler: Registering RDD 38 (collect at StringIndexer.scala:204) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,019 INFO scheduler.DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,020 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,020 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,020 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,024 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,035 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,037 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,037 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,038 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,038 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,038 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,039 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,056 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:33579 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,092 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,217 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 178 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,217 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,218 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0.193 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,219 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,219 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,219 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,219 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,272 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,274 INFO scheduler.DAGScheduler: Got job 11 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,275 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,275 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,275 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,276 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,279 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,282 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,283 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,283 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,284 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,284 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,286 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,301 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:33579 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,307 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.147.198:48300\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,340 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,340 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,341 INFO scheduler.DAGScheduler: ResultStage 17 (collect at StringIndexer.scala:204) finished in 0.064 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,344 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,345 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,345 INFO scheduler.DAGScheduler: Job 11 finished: collect at StringIndexer.scala:204, took 0.072984 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,438 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,439 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,439 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,456 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 317.5 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,481 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,481 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,482 INFO spark.SparkContext: Created broadcast 18 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,483 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,484 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,487 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:33579 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,492 INFO scheduler.DAGScheduler: Registering RDD 45 (collect at StringIndexer.scala:204) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,493 INFO scheduler.DAGScheduler: Got map stage job 12 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,493 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,493 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,494 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,494 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,501 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,533 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,537 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,538 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,538 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,541 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,541 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,549 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,559 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:33579 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,560 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,572 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:33579 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,587 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,588 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:33579 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,610 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,630 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,638 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:33579 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,654 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,657 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:33579 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,676 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:33579 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,697 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,722 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 173 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,722 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,723 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (collect at StringIndexer.scala:204) finished in 0.228 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,723 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,723 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,723 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,723 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,752 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,753 INFO scheduler.DAGScheduler: Got job 13 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,753 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,753 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,753 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,753 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,755 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,410 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,410 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,441 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 7). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,605 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,606 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,615 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,642 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,644 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 29 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,645 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,649 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,651 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,661 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,665 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,678 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,726 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,755 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 8). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,804 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,805 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,806 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,807 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,813 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,816 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,817 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,822 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,822 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,826 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,827 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,827 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:08,866 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 9). 3516 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,043 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,044 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,046 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,054 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,057 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,058 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,074 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,082 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,090 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,093 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,126 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,169 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,214 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 10). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,289 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,289 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,290 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,292 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,299 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,302 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,303 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,307 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,307 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,309 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,310 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,310 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,337 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 11). 3524 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,758 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,759 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,759 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,760 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,760 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,761 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,773 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:33579 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,779 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.147.198:48300\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,811 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 13) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,811 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,812 INFO scheduler.DAGScheduler: ResultStage 20 (collect at StringIndexer.scala:204) finished in 0.058 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,813 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,814 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,815 INFO scheduler.DAGScheduler: Job 13 finished: collect at StringIndexer.scala:204, took 0.062746 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,930 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,931 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,931 INFO datasources.FileSourceStrategy: Output Data Schema: struct<native_country: string>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,948 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,960 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,960 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,961 INFO spark.SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,963 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,963 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,968 INFO scheduler.DAGScheduler: Registering RDD 52 (collect at StringIndexer.scala:204) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,969 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,969 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,969 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,969 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,971 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,985 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,987 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,987 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.147.198:33175 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,988 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,988 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,988 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:09,989 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,000 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:37291 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,013 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:37291 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m04-18 01:29 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1744939709972_0001.inprogress\u001b[0m\n",
      "\u001b[34m04-18 01:29 root         INFO     copying /tmp/spark-events/application_1744939709972_0001.inprogress to /opt/ml/processing/spark-events/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,146 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 157 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,146 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,147 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (collect at StringIndexer.scala:204) finished in 0.175 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,147 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,147 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,148 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,148 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,169 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,170 INFO scheduler.DAGScheduler: Got job 15 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,170 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,170 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,170 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,170 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,176 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,177 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,177 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.147.198:33175 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,178 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,178 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,178 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,180 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,190 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:37291 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,197 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.168.83:39498\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,229 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 50 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,229 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,230 INFO scheduler.DAGScheduler: ResultStage 23 (collect at StringIndexer.scala:204) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,230 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,230 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,231 INFO scheduler.DAGScheduler: Job 15 finished: collect at StringIndexer.scala:204, took 0.061452 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,254 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:33579 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,261 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,279 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-2:37291 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,288 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,303 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:33579 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,320 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,325 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.147.198:33175 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,327 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:33579 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,332 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.147.198:33175 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,334 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-2:37291 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:03,992 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:04,020 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 3). 3604 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:09,991 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:09,992 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 14)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:09,993 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:09,994 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:09,999 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,001 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,002 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,005 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,007 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,012 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,014 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,022 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,112 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,144 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 14). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,182 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,182 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 15)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,183 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,552 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,553 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,555 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,570 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,577 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 21 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,578 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,582 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,588 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,608 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,611 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 22 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,631 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,672 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,717 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 12). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,763 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,764 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,765 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,766 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,771 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,812 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,897 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,897 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,898 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,981 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-2:37291 in memory (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:10,984 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.147.198:33175 in memory (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,493 INFO codegen.CodeGenerator: Code generated in 317.317639 ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,497 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,510 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,510 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,511 INFO spark.SparkContext: Created broadcast 24 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,512 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,512 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,610 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,611 INFO scheduler.DAGScheduler: Got job 16 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,611 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,611 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,611 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,612 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,619 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,621 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,622 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.147.198:33175 (size: 60.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,622 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,623 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,623 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,625 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 5080 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:11,637 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:37291 (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,344 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:37291 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,184 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,189 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,191 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,192 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,196 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,196 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,198 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,199 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,199 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:10,226 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 15). 3726 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,627 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,627 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 16)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,629 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,636 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,637 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:11,639 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:12,120 INFO codegen.CodeGenerator: Code generated in 279.265519 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:12,272 INFO codegen.CodeGenerator: Code generated in 79.704003 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stderr] 2025-04-18 01:29:12,299 INFO codegen.CodeGenerator: Code generated in 8.125506 ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,767 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 1143 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,767 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,767 INFO scheduler.DAGScheduler: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) finished in 1.154 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,768 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,768 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,768 INFO scheduler.DAGScheduler: Job 16 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108, took 1.158546 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,928 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-2:37291 in memory (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:12,939 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.147.198:33175 in memory (size: 60.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,318 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,318 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,318 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,549 INFO codegen.CodeGenerator: Code generated in 149.090328 ms\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,552 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 317.5 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,561 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,561 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.147.198:33175 (size: 30.3 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,562 INFO spark.SparkContext: Created broadcast 26 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,563 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,563 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,585 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,585 INFO scheduler.DAGScheduler: Got job 17 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,585 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,585 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,586 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,586 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,592 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 1027.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,594 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 1027.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,594 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.147.198:33175 (size: 60.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,595 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,595 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,595 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,596 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 5080 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:13,611 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:33579 (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,352 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:33579 (size: 30.3 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,216 INFO launcher.ContainerLaunch: Container container_1744939709972_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,228 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,230 INFO launcher.ContainerCleanup: Cleaning up container container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,233 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,236 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,239 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,239 INFO application.ApplicationImpl: Removing container_1744939709972_0001_01_000003 from application application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,240 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1744939709972_0001_01_000003\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,240 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/container_tokens\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/container_tokens]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/sysfs\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,249 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000003/sysfs]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,774 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,775 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,778 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,778 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.147.198:41025)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,781 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,782 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,782 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:09,809 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 13). 3428 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,599 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,600 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,601 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,602 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,609 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 60.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,612 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:13,613 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:14,114 INFO codegen.CodeGenerator: Code generated in 274.55179 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:14,280 INFO codegen.CodeGenerator: Code generated in 94.32083 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:14,309 INFO codegen.CodeGenerator: Code generated in 9.561871 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stderr] 2025-04-18 01:29:14,324 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3://labdatabucket-us-west-2-736570222/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,766 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 1170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,766 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,767 INFO scheduler.DAGScheduler: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) finished in 1.179 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,767 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,767 INFO cluster.YarnScheduler: Killing all running tasks in stage 25: Stage finished\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,767 INFO scheduler.DAGScheduler: Job 17 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110, took 1.182366 s\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,814 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:33579 in memory (size: 60.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:14,824 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.147.198:33175 in memory (size: 60.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,089 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,096 INFO server.AbstractConnector: Stopped Spark@50afd9a7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,099 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.147.198:4040\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,103 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,118 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,118 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,123 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,138 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,155 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@37b472b7.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,156 INFO nio.NioEventLoop: Migrated 1 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,157 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@3a771aa8.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,157 INFO nio.NioEventLoop: Migrated 0 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,165 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,167 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,174 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,182 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,199 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,199 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,200 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f94a86ad-15a5-49aa-88de-f1448c071264\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,203 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f94a86ad-15a5-49aa-88de-f1448c071264/pyspark-dcfc4b4e-9a34-45ea-ab1a-4ad4d501fc74\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,206 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ce7a98a3-d567-438f-b5d9-94308cf1d05f\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,217 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1744939709972_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,217 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,217 INFO rmapp.RMAppImpl: Updating application application_1744939709972_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,218 INFO recovery.RMStateStore: Updating info for app: application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,218 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,218 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,219 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,219 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,220 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,221 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,242 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,242 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,281 INFO launcher.ContainerLaunch: Container container_1744939709972_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,289 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,290 INFO launcher.ContainerCleanup: Cleaning up container container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,292 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,291 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,294 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,294 INFO application.ApplicationImpl: Removing container_1744939709972_0001_01_000002 from application application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,295 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1744939709972_0001_01_000002\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,295 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,297 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,297 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,307 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,307 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,307 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,308 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,308 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,308 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,317 INFO resourcemanager.ApplicationMasterService: application_1744939709972_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m04-18 01:29 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,679 INFO rmcontainer.RMContainerImpl: container_1744939709972_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,679 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,679 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000001#011RESOURCE=<memory:896, max memory:15892, vCores:1, max vCores:4>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,680 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,682 INFO attempt.RMAppAttemptImpl: appattempt_1744939709972_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,685 INFO rmapp.RMAppImpl: application_1744939709972_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,685 INFO capacity.CapacityScheduler: Application Attempt appattempt_1744939709972_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,685 INFO scheduler.AppSchedulingInfo: Application application_1744939709972_0001 requests cleared\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,686 INFO capacity.LeafQueue: Application removed - appId: application_1744939709972_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,686 INFO capacity.ParentQueue: Application removed - appId: application_1744939709972_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,686 INFO amlauncher.AMLauncher: Cleaning master appattempt_1744939709972_0001_000001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,687 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:15,688 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1744939709972_0001,name=PySparkApp,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1744939709972_0001/,appMasterHost=10.0.168.83,submitTime=1744939724254,startTime=1744939724309,launchTime=1744939725038,finishTime=1744939755217,finalStatus=SUCCEEDED,memorySeconds=645233,vcoreSeconds=75,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=645233 MB-seconds\\, 75 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:28:59.733+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->7841K(66560K)] 57344K->7849K(218112K), 0.0074923 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:00.263+0000: [GC (Allocation Failure) [PSYoungGen: 65185K->7451K(66560K)] 65193K->7467K(218112K), 0.0097579 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:00.434+0000: [GC (Metadata GC Threshold) [PSYoungGen: 48282K->6849K(66560K)] 48298K->6873K(218112K), 0.0049192 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:00.439+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 6849K->0K(66560K)] [ParOldGen: 24K->6669K(91136K)] 6873K->6669K(157696K), [Metaspace: 20403K->20403K(1067008K)], 0.0260224 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:00.834+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->4326K(92160K)] 64013K->11003K(183296K), 0.0047581 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:01.188+0000: [GC (Allocation Failure) [PSYoungGen: 91878K->6747K(119296K)] 98555K->13432K(210432K), 0.0066073 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:01.352+0000: [GC (Metadata GC Threshold) [PSYoungGen: 46308K->5271K(142848K)] 52993K->11957K(233984K), 0.0061998 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:01.359+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 5271K->0K(142848K)] [ParOldGen: 6685K->10050K(137216K)] 11957K->10050K(280064K), [Metaspace: 33897K->33897K(1079296K)], 0.0278394 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:04.585+0000: [GC (Allocation Failure) [PSYoungGen: 134137K->8680K(142848K)] 144187K->29680K(280064K), 0.0205012 secs] [Times: user=0.03 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:04.786+0000: [GC (Metadata GC Threshold) [PSYoungGen: 40515K->10466K(209408K)] 61514K->31473K(346624K), 0.0107594 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:04.797+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 10466K->0K(209408K)] [ParOldGen: 21007K->17876K(185344K)] 31473K->17876K(394752K), [Metaspace: 56000K->56000K(1101824K)], 0.0782896 secs] [Times: user=0.18 sys=0.00, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:07.128+0000: [GC (Allocation Failure) [PSYoungGen: 194048K->15354K(209408K)] 211924K->36751K(394752K), 0.0181900 secs] [Times: user=0.03 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] 2025-04-18T01:29:13.695+0000: [GC (Allocation Failure) [PSYoungGen: 209402K->16779K(262144K)] 230799K->38184K(447488K), 0.0195519 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:16,301 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:16,301 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:16,301 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:16,302 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:16,302 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1744939709972_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/use[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:54.406+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->7449K(66560K)] 57344K->7457K(218112K), 0.0107004 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:54.862+0000: [GC (Allocation Failure) [PSYoungGen: 64793K->7075K(123904K)] 64801K->7091K(275456K), 0.0101153 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:55.118+0000: [GC (Metadata GC Threshold) [PSYoungGen: 46294K->7364K(123904K)] 46310K->7388K(275456K), 0.0073422 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:55.126+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7364K->0K(123904K)] [ParOldGen: 24K->7155K(89088K)] 7388K->7155K(212992K), [Metaspace: 20449K->20449K(1067008K)], 0.0255097 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:55.822+0000: [GC (Allocation Failure) [PSYoungGen: 114688K->8109K(185856K)] 121843K->15273K(274944K), 0.0078344 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:56.122+0000: [GC (Metadata GC Threshold) [PSYoungGen: 77435K->6286K(237568K)] 84599K->13458K(326656K), 0.0078003 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:56.129+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 6286K->0K(237568K)] [ParOldGen: 7171K->11096K(136192K)] 13458K->11096K(373760K), [Metaspace: 33883K->33880K(1079296K)], 0.0283290 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:59.207+0000: [GC (Metadata GC Threshold) [PSYoungGen: 186226K->8679K(279552K)] 197323K->33351K(415744K), 0.0342159 secs] [Times: user=0.09 sys=0.01, real=0.04 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:28:59.241+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8679K->0K(279552K)] [ParOldGen: 24672K->29209K(206336K)] 33351K->29209K(485888K), [Metaspace: 56002K->56002K(1101824K)], 0.1022491 secs] [Times: user=0.26 sys=0.01, real=0.10 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:29:01.968+0000: [GC (Allocation Failure) [PSYoungGen: 270848K->16884K(287744K)] 300057K->49869K(494080K), 0.0209534 secs] [Times: user=0.03 sys=0.02, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] 2025-04-18T01:29:12.454+0000: [GC (Allocation Failure) [PSYoungGen: 287732K->18764K(377856K)] 320717K->51757K(584192K), 0.0261779 secs] [Times: user=0.07 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout] Heap\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout]  PSYoungGen      total 377856K, used 194163K [0x00000006bdb00000, 0x00000006d6f00000, 0x00000007c0000000)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout]   eden space 358912K, 48% used [0x00000006bdb00000,0x00000006c8649ea8,0x00000006d3980000)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000003/stdout]   from space 18944K, 99% used [0x00000006d5c80000,0x00000006d6ed3108,0x00000006d6f00000)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,674 INFO launcher.ContainerLaunch: Container container_1744939709972_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,674 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,674 INFO launcher.ContainerCleanup: Cleaning up container container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,675 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,675 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,676 INFO container.ContainerImpl: Container container_1744939709972_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,676 INFO application.ApplicationImpl: Removing container_1744939709972_0001_01_000001 from application application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,677 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,677 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,689 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,689 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,689 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/container_tokens\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,690 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/container_tokens]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,690 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/sysfs\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,690 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001/container_1744939709972_0001_01_000001/sysfs]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,696 INFO ipc.Server: Auth successful for appattempt_1744939709972_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,699 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:15,699 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.147.198#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1744939709972_0001#011CONTAINERID=container_1744939709972_0001_01_000001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,681 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1744939709972_0001_01_000001]\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,683 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,684 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,684 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1744939709972_0001\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,685 INFO application.ApplicationImpl: Application application_1744939709972_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:16,685 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1744939709972_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,263 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,264 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,264 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,264 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,264 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,265 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,265 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266131\n",
      "  getBytesOnDisk()  = 266131\n",
      "  getVisibleLength()= 266131\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,280 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,488 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,957 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,963 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,963 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,963 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m2025-04-18 01:29:18,964 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,650 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,651 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,652 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,652 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,652 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,652 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,653 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266131\n",
      "  getBytesOnDisk()  = 266131\n",
      "  getVisibleLength()= 266131\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,668 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:20,696 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:21,164 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:21,170 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:21,170 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:21,170 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:21,171 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1149909736-10.0.147.198-1744939702274 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1149909736-10.0.147.198-1744939702274/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[34m04-18 01:29 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m04-18 01:29 root         INFO     copying /tmp/spark-events/application_1744939709972_0001 to /opt/ml/processing/spark-events/application_1744939709972_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1744939709972_0001/container_1744939709972_0001_01_000002/stdout]  PSY\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:30,707 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.168.83\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:32,650 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.168.83\"; destination host is: \"algo-1\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.EOFException\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:392)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)\u001b[0m\n",
      "\u001b[35m04-18 01:29 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc0a43a12b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m04-18 01:29 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc0a43a1430>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:36,650 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m04-18 01:29 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc0a43a1670>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:37,650 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:38,651 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:39,652 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:40,652 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m04-18 01:29 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc0a43a1880>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:41,653 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:42,654 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:43,654 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:44,655 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:45,656 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:45,657 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.0.168.83 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1435)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:47,658 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2025-04-18 01:29:48,659 INFO ipc.Client: Retrying connect to server: algo-1/10.0.147.198:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m04-18 01:29 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc0a43a1a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m04-18 01:29 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1744939709972_0001/container_174493\u001b[0m\n",
      "\n",
      "Spark Processing Job Completed.\n"
     ]
    }
   ],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app=\"pyspark_preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Validate the data processing results\n",
    "\n",
    "Validate the output of the data processing job that you ran by reviewing the first five rows of the train and validation output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://labdatabucket-us-west-2-736570222/data/output/train/\n",
      "\"(84,[3,10,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,143331.0,7.0,40.0])\"\n",
      "\"(84,[3,12,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,165361.0,6.0,40.0])\"\n",
      "\"(84,[0,10,22,32,42,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,28544.0,7.0,20.0])\"\n",
      "\"(84,[0,10,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,40299.0,7.0,25.0])\"\n",
      "\"(84,[0,12,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,190941.0,6.0,20.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://labdatabucket-us-west-2-736570222/data/output/validation/\n",
      "\"(84,[0,10,22,37,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,61838.0,7.0,40.0])\"\n",
      "\"(84,[0,7,22,30,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,54440.0,10.0,20.0])\"\n",
      "\"(84,[0,7,22,32,43,48,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,90860.0,10.0,20.0])\"\n",
      "\"(84,[0,19,22,40,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,96869.0,8.0,40.0])\"\n",
      "\"(84,[0,10,22,30,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,161245.0,7.0,15.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Processing to successfully create a Spark processing job using the SageMaker Python SDK and run a processing job.\n",
    "\n",
    "The next task of the lab focuses on data processing with SageMaker Processing and the built-in scikit-learn container.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 3: Perform data processing with SageMaker Processing and the built-in scikit-learn container**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
